<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Home - FIN-X</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>

<nav class="navbar">
  <a href="index.html">
    <div class="logo">
      <img src="images/finx_white.png" alt="Logo" class="logo-img" />
    </div>
  </a>
  <button class="menu-toggle" aria-label="Toggle navigation">
    <span class="hamburger"></span>
  </button>
  <ul class="nav-links">
    <li><a href="index.html"><b>Home</b></a></li>
    <li><a href="about.html">About</a></li>
    <li><a href="approach.html">Approach</a></li>
    <li><a href="prototypes.html">Prototypes</a></li>
    <li><a href="publications.html">Publications</a></li>
    <li><a href="funding.html">Funding</a></li>
    <li><a href="partners.html">Partners</a></li>
    <li><a href="contact.html">Contact</a></li>
  </ul>
</nav>

<div class="main-content">
  <section id="home">
    <h2>FIN-X - Designing Meaningful Explanations</h2><br>

    <p>AI is playing an increasingly important role in the financial sector, helping institutions make faster, data-driven decisions in areas such as credit assessment and fraud detection. These are complex, high-impact cases where patterns in data aren’t always obvious to the human eye. AI models are capable of uncovering these patterns across vast datasets, supporting teams in reducing risk and improving operational efficiency.</p><br>

    <p>In the research project FIN-X, our focus is not just on what the AI model can do—but on how it supports the person using it. We specifically investigate the role of the internal user: the risk analyst, claims handler, or credit officer who interacts with the model’s output and must make a final decision. Understanding how these professionals interpret explanations, and what they need to trust and act on them, is central to our research.</p><br>

    <p>While these models often perform well from a technical standpoint, the way their results are presented still creates friction. Key reasoning and data are often hidden behind scores or icons, making it hard for users to judge the reliability of the outcome—or to confidently explain it to others.</p><br>

    <p>To address this, we collaborated with Dutch financial institutions to explore how AI-generated explanations can be improved. We tested our designs on two relevant use cases: detecting potential car insurance fraud and assessing business credit applications. These provided concrete scenarios to understand different user needs and tailor our interface solutions accordingly.</p><br>

    <p>The final designs — available on the <a href="prototypes.html">Prototypes</a> page — are based on a fictional but realistic car insurance case, used to evaluate explanation formats in a controlled setting. Through repeated testing and iteration, we developed a set of practical strategies for more user-centered and transparent AI systems.</p><br>

    <p>Below, we highlight four design insights that helped improve how explanations are communicated, based on our applied research and interface work in the field.</p><br>

    <b>1. Use Neutral Colors Thoughtfully</b><br>
    <p>Red is often perceived as negative, green as positive. That works in many cases—but when you're dealing with approval scores or risk levels, it’s not always that black-and-white. In our designs, we used sky blue and gray as neutral colors for values that weren’t inherently “good” or “bad.” This helped prevent users from misreading scores as favorable just because they were low.</p><br>

    <p><i>Example:</i> Blue and gray gave users a more neutral baseline. Green, when used only for truly positive signals like “Approved,” became more meaningful and easier to spot at a glance.</p><br>

    <figure>
      <div class="image-group">
        <img src="images/about/about_ce3.png" alt="Fraud case UI version A" />
        <img src="images/about/about_4.png" alt="Fraud case UI version B" />
      </div>
      <figcaption>Blue and gray communicate neutrality more effectively than green in ambiguous decisions. Green can be reserved for clearly positive signals to make them stand out more.</figcaption>
    </figure>

    <b>2. Make the Invisible Visible</b><br>
    <p>Many AI interfaces reduce explanations to a single number or icon. But what’s behind that score? Without context, users struggle to trust what they see. We found that showing contributing factors, comparisons, and optional details made a huge difference.</p><br>

    <p><i>Example:</i> Instead of just showing “Risk: 126,” we included where the number came from, how it compared to similar cases, and what properties contributed. This gave users the confidence to act.</p><br>

    <figure>
      <div class="image-group">
        <img src="images/about/about_5.png" alt="Fraud case UI version A" />
        <img src="images/about/about_6.png" alt="Fraud case UI version B" />
      </div>
      <figcaption>Blue and gray communicate neutrality more effectively than green in ambiguous decisions. Green can be reserved for clearly positive signals to make them stand out more.</figcaption>
    </figure>
    <b>3. Keep It Focused</b><br>
    <p>It’s tempting to include every bit of data, but clarity comes from restraint. We prioritized visuals that quickly communicated proportion, ranking, or importance. Users should understand the message without needing to read every number.</p><br>

    <p><i>Example:</i> In one screen, we showed which variables contributed to a score using simple bars and color indicators. The raw values were there too—but most users didn’t need them to get the point.</p><br>

    <figure>
      <div class="image-group">
        <img src="images/about/about_7.png" alt="Fraud case UI version A" />
        <img src="images/about/about_8.png" alt="Fraud case UI version B" />
      </div>
      <figcaption>Blue and gray communicate neutrality more effectively than green in ambiguous decisions. Green can be reserved for clearly positive signals to make them stand out more.</figcaption>
    </figure>

    <b>4. Use Structure and Visual Hierarchy</b><br>
    <p>Realism in UI is not always helpful. What matters more is a clear structure that users can follow quickly. We designed layouts with alignment, consistent spacing, and visual contrast to support scanning from top-left to bottom-right—how most people naturally read.</p><br>

    <p><i>Example:</i> Larger headers and numbers drew attention, while softer text and icons faded into the background until needed. This made users feel less overwhelmed and more in control.</p><br>

    <figure>
      <div class="image-group">
        <img src="images/about/about_9.png" alt="Fraud case UI version A" />
        <img src="images/about/about_10.png" alt="Fraud case UI version B" />
      </div>
      <figcaption>Blue and gray communicate neutrality more effectively than green in ambiguous decisions. Green can be reserved for clearly positive signals to make them stand out more.</figcaption>
    </figure>
    <h3>Conclusion</h3><br>
    <p>Designing for explainable AI means more than visualizing model output—it’s about creating interfaces that users can understand, trust, and learn from. Through careful use of color, context, structure, and clarity, we can make black-box systems more human. For designers and developers working on XAI systems, these lessons offer a practical path forward.</p><br>
  </section>
</div>

<footer>
  <p>FIN-X. All rights reserved.</p>
</footer>

<script src="script.js"></script>
</body>
</html>
