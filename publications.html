<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Publications - FIN-X</title>
  <link rel="stylesheet" href="style.css">
  <style>
    /* Basic styling for publications page */
    .publication-list {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }

    .publication-item {
      margin-bottom: 20px;
      padding: 15px;
      border-bottom: 1px solid #ddd;
    }

    .publication-item h3 {
      margin: 0;
      font-size: 1.2em;
      color: #333;
    }

    .publication-item p {
      margin: 5px 0;
      color: #555;
    }

    .publication-item a {
      color: #0066cc;
      text-decoration: none;
    }

    .publication-item a:hover {
      text-decoration: underline;
    }

    .publication-type {
      display: inline-block;
      background-color: #f0f0f0;
      padding: 4px 8px;
      font-size: 0.9em;
      color: #666;
      border-radius: 4px;
      margin-right: 8px;
    }
  </style>
</head>
<body>

  <nav class="navbar">
    <a href="index.html">
      <div class="logo">
        <img src="images/finx_white.png" alt="Logo" class="logo-img">
      </div>
    </a>
    <ul class="nav-links">
      <li><a href="index.html">Home</a></li>
      <li><a href="about.html">About</a></li>
      <li><a href="approach.html">Approach</a></li>
      <li><a href="prototypes.html">Prototypes</a></li>
      <li><a href="publications.html"><b>Publications</b></a></li>
      <li><a href="funding.html">Funding</a></li>
      <li><a href="partners.html">Partners</a></li>
      <li><a href="contact.html">Contact</a></li>
    </ul>
  </nav>

  <!-- Publications Section -->

  <div class="main-content">
    <section id="publications" class="publication-list">
      <h2>Our Publications</h2><br>
      <p>Explore our collection of white papers, research articles, and reports that highlight the progress and findings of the FIN-X project in explainable AI for the financial sector.</p><br>

      <!-- Sample publication item -->
      <div class="publication-item">
        <h3><a href="https://pure.hva.nl/ws/portalfiles/portal/48927425/HHAI_2024_RAAIT_workshop_2024_07_01-artikel_FIN-X.pdf" target="_blank">Identifying XAI User Needs: Gaps between Literature and Use Cases in the Financial Sector</a></h3>
        <span class="publication-type">Academic Paper</span>
        <p><strong>Author(s):</strong> Jenia Kim, Henry Maathuis, C.A.G.M van Montfort, Danielle Sent</p>
        <p><strong>Published in:</strong> Proceedings of the 2nd Workshop on Responsible Applied Artificial Intelligence</p>

        <p><strong>Abstract:</strong> One aspect of a responsible application of AI is ensuring that the operation and outputs of an AI system are understandable for non-technical users, who need to consider its recommendations in their decision making. The importance of explainable AI (XAI) is widely acknowledged; however, its practical implementation is not straightforward. In particular, it is still unclear what are the requirements of non-technical users from explanations, i.e. what makes an explanation meaningful. In this paper, we synthesize insights about meaningful explanations from a literature study and two use cases in the financial sector. We identify 30 components of meaningfulness in the XAI literature. In addition, we report three aspects that were central to the users in our use cases, but are not prominent in the literature: actionability, coherent narratives and context. Our results highlight the importance of narrowing the gap between theoretical and applied Responsible AI.</p>
      </div>

      <div class="publication-item">
        <h3><a href="https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1456486/full" target="_blank">Human-Centered Evaluation of Explainable AI Applications: a Systematic Review</a></h3>
        <span class="publication-type">Academic Paper</span>
        <p><strong>Author(s):</strong> Jenia Kim, Henry Maathuis, Danielle Sent</p>
        <p><strong>Published in:</strong> Frontiers in Artificial Intelligence</p>
        <p><strong>Abstract:</strong> Explainable Artificial Intelligence (XAI) aims to provide insights into the inner workings and the outputs of AI systems. Recently, there's been growing recognition that explainability is inherently human-centric, tied to how people perceive explanations. Despite this, there is no consensus in the research community on whether user evaluation is crucial in XAI, and if so, what exactly needs to be evaluated and how. This systematic literature review addresses this gap by providing a detailed overview of the current state of affairs in human-centered XAI evaluation. We reviewed 73 papers across various domains where XAI was evaluated with users. These studies assessed what makes an explanation “good” from a user's perspective, i.e., what makes an explanation meaningful to a user of an AI system. We identified 30 components of meaningful explanations that were evaluated in the reviewed papers and categorized them into a taxonomy of human-centered XAI evaluation, based on: (a) the contextualized quality of the explanation, (b) the contribution of the explanation to human-AI interaction, and (c) the contribution of the explanation to human-AI performance. Our analysis also revealed a lack of standardization in the methodologies applied in XAI user studies, with only 19 of the 73 papers applying an evaluation framework used by at least one other study in the sample. These inconsistencies hinder cross-study comparisons and broader insights. Our findings contribute to understanding what makes explanations meaningful to users and how to measure this, guiding the XAI community toward a more unified approach in human-centered explainability..</p>
      </div>

      <div class="publication-item">
        <h3><a href="link-to-paper-3.pdf" target="_blank">Checklist for Human-Centered Evaluation of XAI Systems</a></h3>
        <span class="publication-type">White Paper</span>
        <p><strong>Author(s):</strong> FIN-X Research Team</p>
        <p><strong>Published in:</strong> AGConnect, Hogeschool Utrecht</p>

        <p><strong>Summary:</strong> Provides a practical checklist for evaluating Explainable AI systems with internal users.</p>
      </div>

      <!-- Add more publication items as needed -->
    </section>
  </div>

  <footer>
    <p>FIN-X. All rights reserved.</p>
  </footer>

</body>
</html>
