<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>About - FIN-X</title>
  <link rel="stylesheet" href="style.css">
  <style>
    /* Remove bullet points and add spacing for better readability */
    ul {
      list-style-type: none; /* Remove bullet points */
      padding: 0; /* Remove default padding */
    }
    
    li {
      margin-bottom: 8px; /* Add some space between items */
    }
  </style>
</head>
<body>

  <nav class="navbar">
    <a href="index.html">
      <div class="logo">
        <img src="images/finx_white.png" alt="Logo" class="logo-img">
      </div>
    </a>
    <ul class="nav-links">
      <li><a href="index.html">Home</a></li>
      <li><a href="about.html"><b>About</b></a></li>
      <li><a href="approach.html">Approach</a></li>
      <li><a href="prototypes.html">Prototypes</a></li>
      <li><a href="publications.html">Publications</a></li>
      <li><a href="funding.html">Funding</a></li>
      <li><a href="partners.html">Partners</a></li>
      <li><a href="contact.html">Contact</a></li>
    </ul>
  </nav>

  <!-- About Section -->
  <div class="main-content">
    <section id="about">
      <h2>About the Project</h2><br>
      <p>The explainability of outcomes and processes in artificial intelligence (AI) applications is a crucial factor in fostering trust among consumers and society, especially within the financial sector. In this project, we are developing tools to translate the results of complex AI systems into understandable explanations for internal users, such as risk underwriters and claims handlers. This is important because these employees interact with customers and must be able to provide clear explanations, for example, when a customer's insurance claim is denied due to being flagged as fraudulent by an AI system.</p><br>

      <h3>Objective</h3><br>
      <p>The project aims to make a tangible contribution to the implementation of effective and human-centered AI applications by creating tools that provide internal users with better insights into how these systems work and the reasoning behind their outcomes.</p><br>

      <h3>Results</h3><br>
      <p>The project will deliver guidelines (in the form of tools and instruments) to assist in generating, communicating, and evaluating explanations. The primary target audience for these guidelines includes designers and developers of AI and explainable AI (XAI) systems.</p><br>

      <h3>Explanation Criteria for User Feedback</h3><br>
      <p>We assess the quality of AI explanations based on properties deemed meaningful for internal users of XAI systems.</p><br>
      <ul>
<li><strong>Understandability:</strong> "Is the explanation easy to understand?"</li>
<li><strong>Ease of Understanding:</strong> "How easy is it to understand the explanation?"</li>
<li><strong>Ease of Use:</strong> "How easy is it to interact with or navigate through the explanation?"</li>
<li><strong>Satisfaction:</strong> "How satisfying is the explanation?"</li>
<li><strong>Usefulness:</strong> "How useful is the explanation in understanding the prediction?"</li>
<li><strong>Trust:</strong> "How much can you trust the explanation?"</li>
<li><strong>Typicality:</strong> "Does the explanation resemble what you would typically see for similar predictions?"</li>
<li><strong>Sufficiency:</strong> "Does the explanation provide all the information you need?"</li>
<li><strong>Correctness:</strong> "Is the explanation correct and consistent with the prediction?"</li>
<li><strong>Compactness:</strong> "Is the explanation clear and concise, without unnecessary details?"</li>
<li><strong>Actionability:</strong> "How helpful is the explanation in guiding what to do next?"</li>
      </ul>
    </section>
  </div>

  <footer>
    <p>FIN-X. All rights reserved.</p>
  </footer>

</body>
</html>
